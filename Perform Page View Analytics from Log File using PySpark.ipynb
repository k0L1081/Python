{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sample rows from source file (page-views.csv) **\n",
    "\n",
    "1496253932,/search?q=geophysics,,1689<br>\n",
    "1496253946,/repository/1207,/search?q=geophysics,1689<br>\n",
    "1496253979,/repository/8057,/search?q=geophysics,1689<br>\n",
    "1496229649,/search?q=millstream,,7040<br>\n",
    "1496229668,/repository/8947,/search?q=millstream,7040<br>\n",
    "1496229692,/repository/7436,/search?q=millstream,7040<br>\n",
    "1496229666,/repository/4531,/search?q=millstream,7040<br>\n",
    "1496280938,/search?q=sibyllist,,4190<br>\n",
    "1496280958,/repository/8561,/search?q=sibyllist,4190<br>\n",
    "1496280949,/repository/1748,/search?q=sibyllist,4190<br>\n",
    "1496256457,/search?q=protocanonical,,214<br>\n",
    "1496256488,/repository/4345,/search?q=protocanonical,214<br>\n",
    "1496256490,/repository/2128,/search?q=protocanonical,214<br>\n",
    "1496256514,/repository/9783,/search?q=protocanonical,214<br>\n",
    "1496259900,/search?q=kerchunk,,2340<br>\n",
    "1496259937,/repository/6248,/search?q=kerchunk,2340<br>\n",
    "1496259941,/repository/5532,/search?q=kerchunk,2340<br>\n",
    "1496278788,/search?q=postulate,,1614<br>\n",
    "1496278829,/repository/1357,/search?q=postulate,1614<br>\n",
    "1496278811,/repository/3555,/search?q=postulate,1614<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spinning up Spark cluster ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[20]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>phoenix</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[20] appName=phoenix>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages.\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    sc\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "finally:\n",
    "    print('Spinning up Spark cluster ...')\n",
    "    sc = SparkContext(appName = 'phoenix', master = 'local[20]')\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[20]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>phoenix</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1afdac617b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required for regexp_replace() method\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Required for toDF() method\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1496253932,/search?q=geophysics,,1689',\n",
       " '1496253946,/repository/1207,/search?q=geophysics,1689',\n",
       " '1496253979,/repository/8057,/search?q=geophysics,1689',\n",
       " '1496229649,/search?q=millstream,,7040',\n",
       " '1496229668,/repository/8947,/search?q=millstream,7040',\n",
       " '1496229692,/repository/7436,/search?q=millstream,7040',\n",
       " '1496229666,/repository/4531,/search?q=millstream,7040',\n",
       " '1496280938,/search?q=sibyllist,,4190',\n",
       " '1496280958,/repository/8561,/search?q=sibyllist,4190',\n",
       " '1496280949,/repository/1748,/search?q=sibyllist,4190',\n",
       " '1496256457,/search?q=protocanonical,,214',\n",
       " '1496256488,/repository/4345,/search?q=protocanonical,214',\n",
       " '1496256490,/repository/2128,/search?q=protocanonical,214',\n",
       " '1496256514,/repository/9783,/search?q=protocanonical,214',\n",
       " '1496259900,/search?q=kerchunk,,2340',\n",
       " '1496259937,/repository/6248,/search?q=kerchunk,2340',\n",
       " '1496259941,/repository/5532,/search?q=kerchunk,2340',\n",
       " '1496278788,/search?q=postulate,,1614',\n",
       " '1496278829,/repository/1357,/search?q=postulate,1614',\n",
       " '1496278811,/repository/3555,/search?q=postulate,1614']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import source file.\n",
    "page_views_files_path = 'C:\\Temp\\page-views.csv'\n",
    "rdd_page_views = sc.textFile(page_views_files_path)\n",
    "print(type(rdd_page_views))\n",
    "print(rdd_page_views.count())\n",
    "rdd_page_views.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1496253932', '/search?q=geophysics', '', '1689'],\n",
       " ['1496253946', '/repository/1207', '/search?q=geophysics', '1689'],\n",
       " ['1496253979', '/repository/8057', '/search?q=geophysics', '1689'],\n",
       " ['1496229649', '/search?q=millstream', '', '7040'],\n",
       " ['1496229668', '/repository/8947', '/search?q=millstream', '7040'],\n",
       " ['1496229692', '/repository/7436', '/search?q=millstream', '7040'],\n",
       " ['1496229666', '/repository/4531', '/search?q=millstream', '7040'],\n",
       " ['1496280938', '/search?q=sibyllist', '', '4190'],\n",
       " ['1496280958', '/repository/8561', '/search?q=sibyllist', '4190'],\n",
       " ['1496280949', '/repository/1748', '/search?q=sibyllist', '4190'],\n",
       " ['1496256457', '/search?q=protocanonical', '', '214'],\n",
       " ['1496256488', '/repository/4345', '/search?q=protocanonical', '214'],\n",
       " ['1496256490', '/repository/2128', '/search?q=protocanonical', '214'],\n",
       " ['1496256514', '/repository/9783', '/search?q=protocanonical', '214'],\n",
       " ['1496259900', '/search?q=kerchunk', '', '2340'],\n",
       " ['1496259937', '/repository/6248', '/search?q=kerchunk', '2340'],\n",
       " ['1496259941', '/repository/5532', '/search?q=kerchunk', '2340'],\n",
       " ['1496278788', '/search?q=postulate', '', '1614'],\n",
       " ['1496278829', '/repository/1357', '/search?q=postulate', '1614'],\n",
       " ['1496278811', '/repository/3555', '/search?q=postulate', '1614']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each line of RDD, split the line by ','; creating single list for each line.\n",
    "# toDF() is only able to consume collection of rows, where each row is itself a collection.\n",
    "rdd_page_views_2 = rdd_page_views.map(lambda i: i.split(','))\n",
    "print(type(rdd_page_views_2))\n",
    "print(rdd_page_views_2.count())\n",
    "rdd_page_views_2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1496253932', 'geophysics', '', '1689'],\n",
       " ['1496253946', '/repository/1207', '/search?q=geophysics', '1689'],\n",
       " ['1496253979', '/repository/8057', '/search?q=geophysics', '1689'],\n",
       " ['1496229649', 'millstream', '', '7040'],\n",
       " ['1496229668', '/repository/8947', '/search?q=millstream', '7040'],\n",
       " ['1496229692', '/repository/7436', '/search?q=millstream', '7040'],\n",
       " ['1496229666', '/repository/4531', '/search?q=millstream', '7040'],\n",
       " ['1496280938', 'sibyllist', '', '4190'],\n",
       " ['1496280958', '/repository/8561', '/search?q=sibyllist', '4190'],\n",
       " ['1496280949', '/repository/1748', '/search?q=sibyllist', '4190'],\n",
       " ['1496256457', 'protocanonical', '', '214'],\n",
       " ['1496256488', '/repository/4345', '/search?q=protocanonical', '214'],\n",
       " ['1496256490', '/repository/2128', '/search?q=protocanonical', '214'],\n",
       " ['1496256514', '/repository/9783', '/search?q=protocanonical', '214'],\n",
       " ['1496259900', 'kerchunk', '', '2340'],\n",
       " ['1496259937', '/repository/6248', '/search?q=kerchunk', '2340'],\n",
       " ['1496259941', '/repository/5532', '/search?q=kerchunk', '2340'],\n",
       " ['1496278788', 'postulate', '', '1614'],\n",
       " ['1496278829', '/repository/1357', '/search?q=postulate', '1614'],\n",
       " ['1496278811', '/repository/3555', '/search?q=postulate', '1614']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For illustration purposes only.\n",
    "rdd_page_views_3 = rdd_page_views_2.map(lambda i: [i[0], i[1].replace('/search?q=',''), i[2], i[3]])\n",
    "print(type(rdd_page_views_3))\n",
    "print(rdd_page_views_3.count())\n",
    "rdd_page_views_3.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "13860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1496253932',\n",
       " 'geophysics',\n",
       " '',\n",
       " '1689',\n",
       " '1496253946',\n",
       " '/repository/1207',\n",
       " '/search?q=geophysics',\n",
       " '1689',\n",
       " '1496253979',\n",
       " '/repository/8057',\n",
       " '/search?q=geophysics',\n",
       " '1689',\n",
       " '1496229649',\n",
       " 'millstream',\n",
       " '',\n",
       " '7040',\n",
       " '1496229668',\n",
       " '/repository/8947',\n",
       " '/search?q=millstream',\n",
       " '7040']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For illustration purposes only.\n",
    "rdd_page_views_4 = rdd_page_views_2.flatMap(lambda i: [i[0], i[1].replace('/search?q=',''), i[2], i[3]])\n",
    "print(type(rdd_page_views_4))\n",
    "print(rdd_page_views_4.count())\n",
    "rdd_page_views_4.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+--------------------+--------------------+----+\n",
      "| timestamp|                path|            referrer| cid|\n",
      "+----------+--------------------+--------------------+----+\n",
      "|1496253932|/search?q=geophysics|                    |1689|\n",
      "|1496253946|    /repository/1207|/search?q=geophysics|1689|\n",
      "|1496253979|    /repository/8057|/search?q=geophysics|1689|\n",
      "|1496229649|/search?q=millstream|                    |7040|\n",
      "|1496229668|    /repository/8947|/search?q=millstream|7040|\n",
      "|1496229692|    /repository/7436|/search?q=millstream|7040|\n",
      "|1496229666|    /repository/4531|/search?q=millstream|7040|\n",
      "|1496280938| /search?q=sibyllist|                    |4190|\n",
      "|1496280958|    /repository/8561| /search?q=sibyllist|4190|\n",
      "|1496280949|    /repository/1748| /search?q=sibyllist|4190|\n",
      "|1496256457|/search?q=protoca...|                    | 214|\n",
      "|1496256488|    /repository/4345|/search?q=protoca...| 214|\n",
      "|1496256490|    /repository/2128|/search?q=protoca...| 214|\n",
      "|1496256514|    /repository/9783|/search?q=protoca...| 214|\n",
      "|1496259900|  /search?q=kerchunk|                    |2340|\n",
      "|1496259937|    /repository/6248|  /search?q=kerchunk|2340|\n",
      "|1496259941|    /repository/5532|  /search?q=kerchunk|2340|\n",
      "|1496278788| /search?q=postulate|                    |1614|\n",
      "|1496278829|    /repository/1357| /search?q=postulate|1614|\n",
      "|1496278811|    /repository/3555| /search?q=postulate|1614|\n",
      "+----------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert the RDD into a DataFrame using the toDF() method.\n",
    "# The execution of this method would include the specification of the following columns, in order:\n",
    "# timestamp, path, referrer, cid\n",
    "df_page_views = rdd_page_views_2.toDF(['timestamp', 'path', 'referrer', 'cid'])\n",
    "print(type(df_page_views))\n",
    "df_page_views.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(timestamp='1496253932', path='/search?q=geophysics', referrer='', cid='1689'),\n",
       " Row(timestamp='1496253946', path='/repository/1207', referrer='/search?q=geophysics', cid='1689'),\n",
       " Row(timestamp='1496253979', path='/repository/8057', referrer='/search?q=geophysics', cid='1689'),\n",
       " Row(timestamp='1496229649', path='/search?q=millstream', referrer='', cid='7040'),\n",
       " Row(timestamp='1496229668', path='/repository/8947', referrer='/search?q=millstream', cid='7040'),\n",
       " Row(timestamp='1496229692', path='/repository/7436', referrer='/search?q=millstream', cid='7040'),\n",
       " Row(timestamp='1496229666', path='/repository/4531', referrer='/search?q=millstream', cid='7040'),\n",
       " Row(timestamp='1496280938', path='/search?q=sibyllist', referrer='', cid='4190'),\n",
       " Row(timestamp='1496280958', path='/repository/8561', referrer='/search?q=sibyllist', cid='4190'),\n",
       " Row(timestamp='1496280949', path='/repository/1748', referrer='/search?q=sibyllist', cid='4190'),\n",
       " Row(timestamp='1496256457', path='/search?q=protocanonical', referrer='', cid='214'),\n",
       " Row(timestamp='1496256488', path='/repository/4345', referrer='/search?q=protocanonical', cid='214'),\n",
       " Row(timestamp='1496256490', path='/repository/2128', referrer='/search?q=protocanonical', cid='214'),\n",
       " Row(timestamp='1496256514', path='/repository/9783', referrer='/search?q=protocanonical', cid='214'),\n",
       " Row(timestamp='1496259900', path='/search?q=kerchunk', referrer='', cid='2340'),\n",
       " Row(timestamp='1496259937', path='/repository/6248', referrer='/search?q=kerchunk', cid='2340'),\n",
       " Row(timestamp='1496259941', path='/repository/5532', referrer='/search?q=kerchunk', cid='2340'),\n",
       " Row(timestamp='1496278788', path='/search?q=postulate', referrer='', cid='1614'),\n",
       " Row(timestamp='1496278829', path='/repository/1357', referrer='/search?q=postulate', cid='1614'),\n",
       " Row(timestamp='1496278811', path='/repository/3555', referrer='/search?q=postulate', cid='1614')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd: Access the RDD on which a DF is based.\n",
    "# Note that RDD, in following example, is collection of Row objects.\n",
    "rdd_page_views_2_2  = df_page_views.rdd\n",
    "\n",
    "print(type(rdd_page_views_2_2))\n",
    "print(rdd_page_views_2_2.count())\n",
    "rdd_page_views_2_2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1496253932', '/search?q=geophysics', '', '1689'),\n",
       " ('1496253946', '/repository/1207', '/search?q=geophysics', '1689'),\n",
       " ('1496253979', '/repository/8057', '/search?q=geophysics', '1689'),\n",
       " ('1496229649', '/search?q=millstream', '', '7040'),\n",
       " ('1496229668', '/repository/8947', '/search?q=millstream', '7040'),\n",
       " ('1496229692', '/repository/7436', '/search?q=millstream', '7040'),\n",
       " ('1496229666', '/repository/4531', '/search?q=millstream', '7040'),\n",
       " ('1496280938', '/search?q=sibyllist', '', '4190'),\n",
       " ('1496280958', '/repository/8561', '/search?q=sibyllist', '4190'),\n",
       " ('1496280949', '/repository/1748', '/search?q=sibyllist', '4190'),\n",
       " ('1496256457', '/search?q=protocanonical', '', '214'),\n",
       " ('1496256488', '/repository/4345', '/search?q=protocanonical', '214'),\n",
       " ('1496256490', '/repository/2128', '/search?q=protocanonical', '214'),\n",
       " ('1496256514', '/repository/9783', '/search?q=protocanonical', '214'),\n",
       " ('1496259900', '/search?q=kerchunk', '', '2340'),\n",
       " ('1496259937', '/repository/6248', '/search?q=kerchunk', '2340'),\n",
       " ('1496259941', '/repository/5532', '/search?q=kerchunk', '2340'),\n",
       " ('1496278788', '/search?q=postulate', '', '1614'),\n",
       " ('1496278829', '/repository/1357', '/search?q=postulate', '1614'),\n",
       " ('1496278811', '/repository/3555', '/search?q=postulate', '1614')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd: Access the RDD on which a DF is based.\n",
    "# Create RDD collection of tuple objects.\n",
    "rdd_page_views_2_3  = df_page_views.rdd.map(tuple)\n",
    "\n",
    "print(type(rdd_page_views_2_3))\n",
    "print(rdd_page_views_2_3.count())\n",
    "rdd_page_views_2_3.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1496253932', '/search?q=geophysics', '', '1689'],\n",
       " ['1496253946', '/repository/1207', '/search?q=geophysics', '1689'],\n",
       " ['1496253979', '/repository/8057', '/search?q=geophysics', '1689'],\n",
       " ['1496229649', '/search?q=millstream', '', '7040'],\n",
       " ['1496229668', '/repository/8947', '/search?q=millstream', '7040'],\n",
       " ['1496229692', '/repository/7436', '/search?q=millstream', '7040'],\n",
       " ['1496229666', '/repository/4531', '/search?q=millstream', '7040'],\n",
       " ['1496280938', '/search?q=sibyllist', '', '4190'],\n",
       " ['1496280958', '/repository/8561', '/search?q=sibyllist', '4190'],\n",
       " ['1496280949', '/repository/1748', '/search?q=sibyllist', '4190'],\n",
       " ['1496256457', '/search?q=protocanonical', '', '214'],\n",
       " ['1496256488', '/repository/4345', '/search?q=protocanonical', '214'],\n",
       " ['1496256490', '/repository/2128', '/search?q=protocanonical', '214'],\n",
       " ['1496256514', '/repository/9783', '/search?q=protocanonical', '214'],\n",
       " ['1496259900', '/search?q=kerchunk', '', '2340'],\n",
       " ['1496259937', '/repository/6248', '/search?q=kerchunk', '2340'],\n",
       " ['1496259941', '/repository/5532', '/search?q=kerchunk', '2340'],\n",
       " ['1496278788', '/search?q=postulate', '', '1614'],\n",
       " ['1496278829', '/repository/1357', '/search?q=postulate', '1614'],\n",
       " ['1496278811', '/repository/3555', '/search?q=postulate', '1614']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd: Access the RDD on which a DF is based.\n",
    "# Create RDD collection of list objects.\n",
    "rdd_page_views_2_4  = df_page_views.rdd.map(list)\n",
    "\n",
    "print(type(rdd_page_views_2_4))\n",
    "print(rdd_page_views_2_4.count())\n",
    "rdd_page_views_2_4.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                path|\n",
      "+--------------------+\n",
      "|/search?q=geophysics|\n",
      "|/search?q=millstream|\n",
      "| /search?q=sibyllist|\n",
      "|/search?q=protoca...|\n",
      "|  /search?q=kerchunk|\n",
      "| /search?q=postulate|\n",
      "|      /search?q=frab|\n",
      "|    /search?q=lootie|\n",
      "|/search?q=hemosal...|\n",
      "| /search?q=Paganalia|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register df_page_views as temp_page_view temp table.\n",
    "df_page_views.registerTempTable('temp_page_view')\n",
    "\n",
    "# Filter for only issued queries.\n",
    "df_page_views_2 = spark.sql(\"select path from temp_page_view where path like '/search?q=%'\")\n",
    "df_page_views_2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                path|         query|\n",
      "+--------------------+--------------+\n",
      "|/search?q=geophysics|    geophysics|\n",
      "|/search?q=millstream|    millstream|\n",
      "| /search?q=sibyllist|     sibyllist|\n",
      "|/search?q=protoca...|protocanonical|\n",
      "|  /search?q=kerchunk|      kerchunk|\n",
      "| /search?q=postulate|     postulate|\n",
      "|      /search?q=frab|          frab|\n",
      "|    /search?q=lootie|        lootie|\n",
      "|/search?q=hemosal...|   hemosalpinx|\n",
      "| /search?q=Paganalia|     Paganalia|\n",
      "|  /search?q=operette|      operette|\n",
      "|/search?q=coachma...|   coachmaking|\n",
      "|  /search?q=fuelizer|      fuelizer|\n",
      "| /search?q=squamulae|     squamulae|\n",
      "|   /search?q=frilled|       frilled|\n",
      "|/search?q=uningen...| uningeniously|\n",
      "|  /search?q=kerchunk|      kerchunk|\n",
      "|/search?q=trombid...|  trombidiasis|\n",
      "|  /search?q=Sabbatic|      Sabbatic|\n",
      "|  /search?q=rimosely|      rimosely|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove '/search?q=' from path.\n",
    "df_page_views_3 = df_page_views_2.withColumn('query', regexp_replace('path', '/search\\?q=', ''))\n",
    "df_page_views_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                path|         query|\n",
      "+--------------------+--------------+\n",
      "|/search?q=geophysics|    geophysics|\n",
      "|/search?q=millstream|    millstream|\n",
      "| /search?q=sibyllist|     sibyllist|\n",
      "|/search?q=protoca...|protocanonical|\n",
      "|  /search?q=kerchunk|      kerchunk|\n",
      "| /search?q=postulate|     postulate|\n",
      "|      /search?q=frab|          frab|\n",
      "|    /search?q=lootie|        lootie|\n",
      "|/search?q=hemosal...|   hemosalpinx|\n",
      "| /search?q=Paganalia|     Paganalia|\n",
      "|  /search?q=operette|      operette|\n",
      "|/search?q=coachma...|   coachmaking|\n",
      "|  /search?q=fuelizer|      fuelizer|\n",
      "| /search?q=squamulae|     squamulae|\n",
      "|   /search?q=frilled|       frilled|\n",
      "|/search?q=uningen...| uningeniously|\n",
      "|  /search?q=kerchunk|      kerchunk|\n",
      "|/search?q=trombid...|  trombidiasis|\n",
      "|  /search?q=Sabbatic|      Sabbatic|\n",
      "|  /search?q=rimosely|      rimosely|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration purposes only.\n",
    "# The second argument of withColumn must be a \"column expression\".\n",
    "# You can use a lambda UDF as the second argument.\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "udf_get_query = udf(lambda path: path.replace('/search?q=', ''))\n",
    "df_page_views_2_1 = df_page_views_2.withColumn('query', udf_get_query('path'))\n",
    "df_page_views_2_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                path|         query|\n",
      "+--------------------+--------------+\n",
      "|/search?q=geophysics|    geophysics|\n",
      "|/search?q=millstream|    millstream|\n",
      "| /search?q=sibyllist|     sibyllist|\n",
      "|/search?q=protoca...|protocanonical|\n",
      "|  /search?q=kerchunk|      kerchunk|\n",
      "| /search?q=postulate|     postulate|\n",
      "|      /search?q=frab|          frab|\n",
      "|    /search?q=lootie|        lootie|\n",
      "|/search?q=hemosal...|   hemosalpinx|\n",
      "| /search?q=Paganalia|     Paganalia|\n",
      "|  /search?q=operette|      operette|\n",
      "|/search?q=coachma...|   coachmaking|\n",
      "|  /search?q=fuelizer|      fuelizer|\n",
      "| /search?q=squamulae|     squamulae|\n",
      "|   /search?q=frilled|       frilled|\n",
      "|/search?q=uningen...| uningeniously|\n",
      "|  /search?q=kerchunk|      kerchunk|\n",
      "|/search?q=trombid...|  trombidiasis|\n",
      "|  /search?q=Sabbatic|      Sabbatic|\n",
      "|  /search?q=rimosely|      rimosely|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration purposes only.\n",
    "# The second argument of withColumn must be a \"column expression\".\n",
    "# You can use a non-lambda UDF as the second argument.\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_query(path):\n",
    "    return(path.replace('/search?q=', ''))\n",
    "\n",
    "udf_get_query_2 = udf(get_query)\n",
    "\n",
    "df_page_views_2_1 = df_page_views_2.withColumn('query', udf_get_query_2('path'))\n",
    "df_page_views_2_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                path| query|\n",
      "+--------------------+------+\n",
      "|/search?q=geophysics|Hello!|\n",
      "|/search?q=millstream|Hello!|\n",
      "| /search?q=sibyllist|Hello!|\n",
      "|/search?q=protoca...|Hello!|\n",
      "|  /search?q=kerchunk|Hello!|\n",
      "| /search?q=postulate|Hello!|\n",
      "|      /search?q=frab|Hello!|\n",
      "|    /search?q=lootie|Hello!|\n",
      "|/search?q=hemosal...|Hello!|\n",
      "| /search?q=Paganalia|Hello!|\n",
      "|  /search?q=operette|Hello!|\n",
      "|/search?q=coachma...|Hello!|\n",
      "|  /search?q=fuelizer|Hello!|\n",
      "| /search?q=squamulae|Hello!|\n",
      "|   /search?q=frilled|Hello!|\n",
      "|/search?q=uningen...|Hello!|\n",
      "|  /search?q=kerchunk|Hello!|\n",
      "|/search?q=trombid...|Hello!|\n",
      "|  /search?q=Sabbatic|Hello!|\n",
      "|  /search?q=rimosely|Hello!|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration purposes only.\n",
    "# The second argument of withColumn must be a \"column expression\".\n",
    "# To use a literal as the second argument, use the lit() function.\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_page_views_2_3 = df_page_views_2.withColumn('query', lit('Hello!'))\n",
    "df_page_views_2_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               query|count|\n",
      "+--------------------+-----+\n",
      "|                crop|   21|\n",
      "|          entosphere|   19|\n",
      "|             Gaudete|   17|\n",
      "|       contrabandage|   16|\n",
      "|         coachmaking|   16|\n",
      "|            operette|   16|\n",
      "|            kerchunk|   16|\n",
      "|          beaconless|   16|\n",
      "|         hemosalpinx|   16|\n",
      "|             Natalia|   15|\n",
      "|roentgenographically|   15|\n",
      "|       preconfinedly|   15|\n",
      "|        trigonometry|   15|\n",
      "|           Argasidae|   15|\n",
      "|             archaic|   15|\n",
      "|            lederite|   14|\n",
      "|        exsanguinity|   14|\n",
      "|           shavester|   14|\n",
      "|             Toxodon|   14|\n",
      "|               Nasua|   13|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register df_page_views_3 as temp_page_view temp table.\n",
    "df_page_views_3.registerTempTable('temp_page_view')\n",
    "\n",
    "# Retrieve count of each query.\n",
    "df_page_views_4 = spark.sql(\"select query, count(*) as count from temp_page_view group by query order by count(*) desc\")\n",
    "df_page_views_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[21, 19, 17, 16, 15]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve distinct counts into query_counts DataFrame.\n",
    "query_counts = df_page_views_4.select('count').distinct()\n",
    "print(type(query_counts))\n",
    "\n",
    "# Convert query_counts DataFrame into Pandas DataFrame; then into list.\n",
    "top_query_counts = sorted(query_counts.toPandas()['count'], reverse=True)[:5]\n",
    "print(type(top_query_counts))\n",
    "\n",
    "top_query_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21', '19', '17', '16', '15']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert items in top_query_counts list into strings.\n",
    "top_query_counts = [str(count) for count in top_query_counts]\n",
    "top_query_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(21, 19, 17, 16, 15)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct SQL part.\n",
    "top_query_counts_SQL = '(' + ', '.join(top_query_counts) + ')'\n",
    "top_query_counts_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select query, count from temp_page_view where count in (21, 19, 17, 16, 15) order by count desc, query\n",
      "+--------------------+-----+\n",
      "|               query|count|\n",
      "+--------------------+-----+\n",
      "|                crop|   21|\n",
      "|          entosphere|   19|\n",
      "|             Gaudete|   17|\n",
      "|          beaconless|   16|\n",
      "|         coachmaking|   16|\n",
      "|       contrabandage|   16|\n",
      "|         hemosalpinx|   16|\n",
      "|            kerchunk|   16|\n",
      "|            operette|   16|\n",
      "|           Argasidae|   15|\n",
      "|             Natalia|   15|\n",
      "|             archaic|   15|\n",
      "|       preconfinedly|   15|\n",
      "|roentgenographically|   15|\n",
      "|        trigonometry|   15|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register df_page_views_4 as temp_page_view temp table.\n",
    "df_page_views_4.registerTempTable('temp_page_view')\n",
    "\n",
    "# Construct SQL.\n",
    "s_SQL = 'select query, count from temp_page_view where count in ' + top_query_counts_SQL + ' order by count desc, query'\n",
    "print(s_SQL)\n",
    "\n",
    "# Execute SQL and store results in df_page_views_5 DataFrame.\n",
    "df_page_views_5 = spark.sql(s_SQL)\n",
    "df_page_views_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of partitions for df_page_views_5.\n",
    "df_page_views_5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set paths for folder destinations.\n",
    "destination_multiple_files_path = r'C:\\temp\\top-issued-queries-multiple'\n",
    "destination_single_file_path = r'C:\\temp\\top-issued-queries-single'\n",
    "    \n",
    "# Write output to files: Spark will write to multiple files, based on the number of dataframe partitions,\n",
    "# in the spirit of parallelized, distributed processing.\n",
    "df_page_views_5.write.format('csv').save(path = destination_multiple_files_path, mode = 'overwrite')\n",
    "\n",
    "# If the dataframe can fit in master node's memory, consolidate partitions in order to write single file.\n",
    "# coalesce(): Reduce number of DF partitions.\n",
    "df_page_views_5.coalesce(1).write.format('csv').save(path = destination_single_file_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, you can reduce the number of partitions in the DF itself, to say 3.\n",
    "df_page_views_6 = df_page_views_5.coalesce(3)\n",
    "\n",
    "# Display the number of partitions for df_page_views_6.\n",
    "print(df_page_views_6.rdd.getNumPartitions())\n",
    "\n",
    "df_page_views_6.write.format('csv').save(path = destination_single_file_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# repartition() can be used to both reduce and increase the number of DF partitions.\n",
    "# coalesce() is more efficient than repartition(), for reducing partitions, that is.\n",
    "# coalesce() keeps the order of the data and minimizes data movement.\n",
    "# coalesce() combines existing partitions. Its unit of granularity is partition.\n",
    "# coalesce() will not shuffle data from the same partition into separate partitions.\n",
    "\n",
    "# However, repartition() does a full shuffle of the data in order to create equal sized partitions.\n",
    "# Therefore, repartition() can shuffle data from the same partition into separate partitions.\n",
    "# repartition() is not guaranteed to keep the order of the data.\n",
    "df_page_views_6 = df_page_views_5.repartition(2)\n",
    "\n",
    "# Display the number of partitions for df_page_views_6.\n",
    "print(df_page_views_6.rdd.getNumPartitions())\n",
    "\n",
    "df_page_views_6.write.format('csv').save(path = destination_single_file_path, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "df_page_views_7 = df_page_views_6.repartition(1)\n",
    "\n",
    "print(df_page_views_7.rdd.getNumPartitions())\n",
    "\n",
    "# Save to file, including header, in overwrite mode and in CSV format.\n",
    "df_page_views_7.write.save(path = destination_single_file_path, header='True', mode='overwrite', format='csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
